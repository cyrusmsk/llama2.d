# llama2.d
<p align="center">
  <img src="assets/llama_and_dman.png" width="300" height="300" alt="Cute Llama">
</p>

This is the D version of [llama2.c](https://github.com/karpathy/llama2.c) by
Andrej Karpathy. It runs inference for the
[llama2](https://github.com/facebookresearch/llama) model architecture recently
published by Meta.

Initial code was generated by [ctod](https://github.com/dkorpel/ctod) tool and saved as ctod_initial.d

After some small manual adjustments:
- added cast(float*) to calloc and mmap
- because of lack clock_gettime on Darwin OS, I've change this part with MonoTime from core.time
- comment our pragmas for OpenMP
it runs with following commnad:
```sh
ldc2 -O3 --release --boundscheck=off run.d
```

Tested on macOS M1.

## Todo

- \[ \] Make code more iDiomatic
- \[ \] Improve performance
- \[ \] Add Windows support (port win.h/win.c files from original repo)
- \[ \] Parallelize the code with std.parallel and SIMD

## Contributing

Any form of contribution is welcome. Feel free to open an issue or create a
pull request. If you are contributing optimizations, please provide benchmarks
and/or performance comparisons as well as the code to reproduce them.

# Credits
- [Andrej Karpathy](https://github.com/karpathy) for the original llama2.c
  implementation
- [Dennis Korpel](https://github.com/dkorpel/ctod) for great ctod tool
- [cgbur](https://github.com/cgbur/llama2.zig/) for ideas for optimizations and readme structure
